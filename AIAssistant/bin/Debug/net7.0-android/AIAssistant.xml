<?xml version="1.0"?>
<doc>
    <assembly>
        <name>AIAssistant</name>
    </assembly>
    <members>
        <member name="P:AIAssistant.Model.ChatRequest.Model">
            <summary>
            The model to use for this request
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.Messages">
            <summary>
            The messages to send with this Chat Request
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.Temperature">
            <summary>
            What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. It is generally recommend to use this or <see cref="P:AIAssistant.Model.ChatRequest.TopP"/> but not both.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.TopP">
            <summary>
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommend to use this or <see cref="P:AIAssistant.Model.ChatRequest.Temperature"/> but not both.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.NumChoicesPerMessage">
            <summary>
            How many different choices to request for each message. Defaults to 1.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.Stream">
            <summary>
            Specifies where the results should stream and be returned at one time.  Do not set this yourself, use the appropriate methods on <see cref="!:CompletionEndpoint"/> instead.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.CompiledStop">
            <summary>
            This is only used for serializing the request into JSON, do not use it directly.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.MultipleStopSequences">
            <summary>
            One or more sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.StopSequence">
            <summary>
            The stop sequence where the API will stop generating further tokens. The returned text will not contain the stop sequence.  For convenience, if you are only requesting a single stop sequence, set it here
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.MaxTokens">
            <summary>
            How many tokens to complete to. Can return fewer if a stop sequence is hit.  Defaults to 16.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.FrequencyPenalty">
            <summary>
            The scale of the penalty for how often a token is used.  Should generally be between 0 and 1, although negative numbers are allowed to encourage token reuse.  Defaults to 0.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.PresencePenalty">
            <summary>
            The scale of the penalty applied if a token is already present at all.  Should generally be between 0 and 1, although negative numbers are allowed to encourage token reuse.  Defaults to 0.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.LogitBias">
            <summary>
            Modify the likelihood of specified tokens appearing in the completion.
            Accepts a json object that maps tokens(specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
            Mathematically, the bias is added to the logits generated by the model prior to sampling.
            The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatRequest.user">
            <summary>
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
            </summary>
        </member>
        <member name="M:AIAssistant.Model.ChatRequest.#ctor">
            <summary>
            Creates a new, empty <see cref="T:AIAssistant.Model.ChatRequest"/>
            </summary>
        </member>
        <member name="M:AIAssistant.Model.ChatRequest.#ctor(AIAssistant.Model.ChatRequest)">
            <summary>
            Create a new chat request using the data from the input chat request.
            </summary>
            <param name="basedOn"></param>
        </member>
        <member name="T:AIAssistant.Model.CompletionRequest">
            <summary>
            Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
            </summary>
        </member>
        <member name="M:AIAssistant.Model.CompletionRequest.#ctor">
            <summary>
            Initialize the completion request.
            </summary>
            <remarks>
            Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
            </remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionRequest.Model">
            <summary>
            ID of the model to use. You can use the <see href="https://beta.openai.com/docs/api-reference/models/list">List models</see> API to see all of your available models, or see our <see href="https://beta.openai.com/docs/models/overview">Model overview</see> for descriptions of them.
            </summary>
            <remarks>
            </remarks>
        </member>
        <!-- Badly formed XML comment ignored for member "P:AIAssistant.Model.CompletionRequest.Prompt" -->
        <!-- Badly formed XML comment ignored for member "P:AIAssistant.Model.CompletionRequest.MaxTokens" -->
        <member name="P:AIAssistant.Model.CompletionRequest.Temperature">
            <summary>
            What <see href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277">sampling temperature</see> to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
            </summary>
            <remarks>We generally recommend altering this or <c>TopP</c> but not both.</remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionRequest.TopP">
            <summary>
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            </summary>
            <remarks>We generally recommend altering this or <c>Temperature</c> but not both.</remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionRequest.PresencePenalty">
            <summary>
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
            </summary>
            <remarks><seealso href="https://beta.openai.com/docs/api-reference/parameter-details">See more information about frequency and presence penalites.</seealso> </remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionRequest.FrequencyPenalty">
            <summary>
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
            </summary>
            <remarks>
            <seealso href="https://beta.openai.com/docs/api-reference/parameter-details">See more information about frequency and presence penalites.</seealso>
            </remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionResponse.Model">
            <summary>
            Model used to generate the original request. 
            </summary>
            <remarks>        
            </remarks>
        </member>
        <member name="P:AIAssistant.Model.CompletionResponse.Choices">
            <summary>
            Completion choices generated by the GPT-3 API.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.CompletionResponse.Usage">
            <summary>
            Details the number of tokens used to process the completion.
            </summary>
        </member>
        <member name="T:AIAssistant.Model.ChatGPTUsage">
            <summary>
            Detailed breakdown of tokens used to process the completion request.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.ChatGPTChoice.Text">
            <summary>
            Completion text generated by the GPT-3 API.
            </summary>
            <remarks>Returned text does not include the stop sequence.</remarks>
        </member>
        <member name="M:AIAssistant.Model.CorrectionRequest.#ctor">
            <summary>
            Initialize the completion request.
            </summary>
            <remarks>
            Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
            </remarks>
        </member>
        <member name="P:AIAssistant.Model.CorrectionRequest.Model">
            <summary>
            ID of the model to use. You can use the <see href="https://beta.openai.com/docs/api-reference/models/list">List models</see> API to see all of your available models, or see our <see href="https://beta.openai.com/docs/models/overview">Model overview</see> for descriptions of them.
            </summary>
            <remarks>
            </remarks>
        </member>
        <!-- Badly formed XML comment ignored for member "P:AIAssistant.Model.CorrectionRequest.Prompt" -->
        <!-- Badly formed XML comment ignored for member "P:AIAssistant.Model.CorrectionRequest.MaxTokens" -->
        <member name="P:AIAssistant.Model.CorrectionRequest.Temperature">
            <summary>
            What <see href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277">sampling temperature</see> to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
            </summary>
            <remarks>We generally recommend altering this or <c>TopP</c> but not both.</remarks>
        </member>
        <member name="P:AIAssistant.Model.CorrectionRequest.TopP">
            <summary>
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            </summary>
            <remarks>We generally recommend altering this or <c>Temperature</c> but not both.</remarks>
        </member>
        <member name="P:AIAssistant.Model.CorrectionRequest.PresencePenalty">
            <summary>
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
            </summary>
            <remarks><seealso href="https://beta.openai.com/docs/api-reference/parameter-details">See more information about frequency and presence penalites.</seealso> </remarks>
        </member>
        <member name="P:AIAssistant.Model.CorrectionRequest.FrequencyPenalty">
            <summary>
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
            </summary>
            <remarks>
            <seealso href="https://beta.openai.com/docs/api-reference/parameter-details">See more information about frequency and presence penalites.</seealso>
            </remarks>
        </member>
        <member name="P:AIAssistant.Model.Model.OwnedBy">
            <summary>
            The owner of this model.  Generally "openai" is a generic OpenAI model, or the organization if a custom or finetuned model.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.Object">
            <summary>
            The type of object. Should always be 'model'.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.Created">
            The time when the model was created
        </member>
        <member name="P:AIAssistant.Model.Model.CreatedUnixTime">
            <summary>
            The time when the model was created in unix epoch format
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.Permission">
            <summary>
            Permissions for use of the model
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.Root">
            <summary>
            Currently (2023-01-27) seems like this is duplicate of <see cref="P:AIAssistant.Model.Model.ModelID"/> but including for completeness.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.Parent">
            <summary>
            Currently (2023-01-27) seems unused, probably intended for nesting of models in a later release
            </summary>
        </member>
        <member name="M:AIAssistant.Model.Model.op_Implicit(AIAssistant.Model.Model)~System.String">
            <summary>
            Allows an model to be implicitly cast to the string of its <see cref="P:AIAssistant.Model.Model.ModelID"/>
            </summary>
            <param name="model">The <see cref="T:AIAssistant.Model.Model"/> to cast to a string.</param>
        </member>
        <member name="M:AIAssistant.Model.Model.op_Implicit(System.String)~AIAssistant.Model.Model">
            <summary>
            Allows a string to be implicitly cast as an <see cref="T:AIAssistant.Model.Model"/> with that <see cref="P:AIAssistant.Model.Model.ModelID"/>
            </summary>
            <param name="name">The id/<see cref="P:AIAssistant.Model.Model.ModelID"/> to use</param>
        </member>
        <member name="M:AIAssistant.Model.Model.#ctor(System.String)">
            <summary>
            Represents an Model with the given id/<see cref="P:AIAssistant.Model.Model.ModelID"/>
            </summary>
            <param name="name">The id/<see cref="P:AIAssistant.Model.Model.ModelID"/> to use.
            </param>
        </member>
        <member name="M:AIAssistant.Model.Model.#ctor">
            <summary>
            Represents a generic Model/model
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.DefaultModel">
            <summary>
            The default model to use in requests if no other model is specified.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.AdaText">
            <summary>
            Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.BabbageText">
            <summary>
            Capable of straightforward tasks, very fast, and lower cost.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.CurieText">
            <summary>
            Very capable, but faster and lower cost than Davinci.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.DavinciText">
            <summary>
            Most capable GPT-3 model. Can do any task the other models can do, often with higher quality, longer output and better instruction-following. Also supports inserting completions within text.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.CushmanCode">
            <summary>
            Almost as capable as Davinci Codex, but slightly faster. This speed advantage may make it preferable for real-time applications.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.DavinciCode">
            <summary>
            Most capable Codex model. Particularly good at translating natural language to code. In addition to completing code, also supports inserting completions within code.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Model.AdaTextEmbedding">
            <summary>
            OpenAI offers one second-generation embedding model for use with the embeddings API endpoint.
            </summary>
        </member>
        <member name="M:AIAssistant.Model.Model.RetrieveModelDetailsAsync(OpenAI_API.OpenAIAPI)">
            <summary>
            Gets more details about this Model from the API, specifically properties such as <see cref="P:AIAssistant.Model.Model.OwnedBy"/> and permissions.
            </summary>
            <param name="api">An instance of the API with authentication in order to call the endpoint.</param>
            <returns>Asynchronously returns an Model with all relevant properties filled in</returns>
        </member>
        <member name="P:AIAssistant.Model.Permission.Id">
            <summary>
            Permission Id (not to be confused with ModelId)
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.Object">
            <summary>
            Object type, should always be 'model_permission'
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.Created">
            The time when the permission was created
        </member>
        <member name="P:AIAssistant.Model.Permission.CreatedUnixTime">
            <summary>
            Unix timestamp for creation date/time
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.AllowCreateEngine">
            <summary>
            Can the model be created?
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.AllowSampling">
            <summary>
            Does the model support temperature sampling?
            https://beta.openai.com/docs/api-reference/completions/create#completions/create-temperature
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.AllowLogProbs">
            <summary>
            Does the model support logprobs?
            https://beta.openai.com/docs/api-reference/completions/create#completions/create-logprobs
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.AllowSearchIndices">
            <summary>
            Does the model support search indices?
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.AllowFineTuning">
            <summary>
            Does the model allow fine tuning?
            https://beta.openai.com/docs/api-reference/fine-tunes
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.Organization">
            <summary>
            Is the model only allowed for a particular organization? May not be implemented yet.
            </summary>
        </member>
        <member name="P:AIAssistant.Model.Permission.Group">
            <summary>
            Is the model part of a group? Seems not implemented yet. Always null.
            </summary>
        </member>
    </members>
</doc>
